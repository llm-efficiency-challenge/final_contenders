FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel

RUN apt-get update  && apt-get install -y git python3-virtualenv wget

COPY LLaMA-Efficient-Tuning /workspace/LLaMA-Efficient-Tuning
COPY upload_model.py ./

# install requirements
WORKDIR /workspace/LLaMA-Efficient-Tuning
RUN pip install -r requirements.txt

# flash attention
WORKDIR /workspace
RUN git clone https://github.com/Dao-AILab/flash-attention.git
RUN cd flash-attention && python setup.py install
RUN cd flash-attention/csrc/layer_norm && pip install .


WORKDIR /workspace/LLaMA-Efficient-Tuning

# RUN bash nips_finetune_flash.sh
CMD ["bash", "nips_finetune_flash.sh"]
