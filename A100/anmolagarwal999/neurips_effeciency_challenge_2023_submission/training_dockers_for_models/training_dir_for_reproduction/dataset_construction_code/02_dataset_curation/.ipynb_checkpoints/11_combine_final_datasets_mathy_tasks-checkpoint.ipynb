{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "363b9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16168fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "30163f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../../data/new_training_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6f063f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a1d9be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(filter(lambda x:(\"combined\" not in x) and ('train' in x) , all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd7f368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "335fc9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pegasus_tqa_train_dataset.json',\n",
       " 'pegasus_cnn_train_dataset.json',\n",
       " 'pegasus_stem_bigbench_train_dataset.json',\n",
       " 'pegasus_bigbench_train_dataset.json',\n",
       " 'pegasus_mathqa_train_dataset.json',\n",
       " 'pegasus_mmlu_train_dataset.json',\n",
       " 'pegasus_gsm_train_dataset.json',\n",
       " 'pegasus_bbq_train_dataset.json']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa1a73",
   "metadata": {},
   "source": [
    "## Construct the MATH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3cf4701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_file(file_name):\n",
    "    path = os.path.join(BASE_DIR, file_name)\n",
    "    with open(path, 'r') as fd:\n",
    "        df = json.load(fd)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f4bd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64ab58",
   "metadata": {},
   "source": [
    "##### GSM8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c89cf170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  6000\n"
     ]
    }
   ],
   "source": [
    "gsm_df = fetch_file('pegasus_gsm_train_dataset.json')[:6000]\n",
    "print(\"Len is: \", len(gsm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66dc70e",
   "metadata": {},
   "source": [
    "##### MathQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d961d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  2580\n"
     ]
    }
   ],
   "source": [
    "mathqa_df = fetch_file('pegasus_mathqa_train_dataset.json')\n",
    "print(\"Len is: \", len(mathqa_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f3ced",
   "metadata": {},
   "source": [
    "##### BigBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c3b436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  1608\n"
     ]
    }
   ],
   "source": [
    "bigbench_df = fetch_file('pegasus_bigbench_train_dataset.json')\n",
    "print(\"Len is: \", len(bigbench_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc3d3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_arr = [\\\n",
    "'logic_grid_puzzle|',\n",
    " #'strange_stories|multiple_choice',\n",
    " #'code_line_description|',\n",
    " 'logical_deduction|three_objects',\n",
    " 'logical_deduction|seven_objects',\n",
    " 'logical_deduction|five_objects',\n",
    " #'analytic_entailment|',\n",
    " #'snarks|',\n",
    " #'empirical_judgments|',\n",
    " #'emoji_movie|',\n",
    " #'logical_fallacy_detection|',\n",
    " #'dark_humor_detection|',\n",
    " #'known_unknowns|',\n",
    " #'causal_judgment|',\n",
    " #'strange_stories|boolean',\n",
    " #'epistemic_reasoning|',\n",
    " #'figure_of_speech_detection|',\n",
    " #'entailed_polarity|',\n",
    " #'tellmewhy|',\n",
    " #'presuppositions_as_nli|',\n",
    " #'formal_fallacies_syllogisms_negation|',\n",
    " #'cause_and_effect|one_sentence'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a35cc27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  919\n"
     ]
    }
   ],
   "source": [
    "bigbench_df = list(filter(lambda x:x['section'] in _use_arr, bigbench_df))\n",
    "print(\"Len is: \", len(bigbench_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96568e35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logic_grid_puzzle|',\n",
       " 'logical_deduction|three_objects',\n",
       " 'logical_deduction|seven_objects',\n",
       " 'logical_deduction|five_objects']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Counter([x['section'] for x in bigbench_df]).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb93b5",
   "metadata": {},
   "source": [
    "##### BigBench STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3089adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  2017\n"
     ]
    }
   ],
   "source": [
    "bigbench_stem_df = fetch_file('pegasus_stem_bigbench_train_dataset.json')\n",
    "print(\"Len is: \", len(bigbench_stem_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d15fe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_arr_2 = [\\\n",
    "'matrixshapes|',\n",
    " 'navigate|',\n",
    " #'vitaminc_fact_verification|',\n",
    " 'physics|',\n",
    " 'unit_conversion|different_systems',\n",
    " 'elementary_math_qa|question_with_mathematical_hint',\n",
    " 'unit_conversion|unit_identification',\n",
    " 'chinese_remainder_theorem|',\n",
    " 'elementary_math_qa|question_with_language_hint',\n",
    " 'elementary_math_qa|question_only',\n",
    " 'physical_intuition|',\n",
    " 'physics_questions|',\n",
    " #'scientific_press_release|',\n",
    " 'elementary_math_qa|mathematical_hint_only',\n",
    " 'mathematical_induction|',\n",
    " 'elementary_math_qa|language_hint_only',\n",
    " #'auto_debugging|'\\\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2639275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  1697\n"
     ]
    }
   ],
   "source": [
    "bigbench_stem_df = list(filter(lambda x:x['section'] in _use_arr_2, bigbench_stem_df))\n",
    "print(\"Len is: \", len(bigbench_stem_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98561bb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrixshapes|',\n",
       " 'navigate|',\n",
       " 'physics|',\n",
       " 'unit_conversion|different_systems',\n",
       " 'elementary_math_qa|question_with_mathematical_hint',\n",
       " 'unit_conversion|unit_identification',\n",
       " 'chinese_remainder_theorem|',\n",
       " 'elementary_math_qa|question_with_language_hint',\n",
       " 'elementary_math_qa|question_only',\n",
       " 'physical_intuition|',\n",
       " 'physics_questions|',\n",
       " 'elementary_math_qa|mathematical_hint_only',\n",
       " 'mathematical_induction|',\n",
       " 'elementary_math_qa|language_hint_only']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Counter([x['section'] for x in bigbench_stem_df]).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ff89e",
   "metadata": {},
   "source": [
    "#### MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f32f0ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  824\n"
     ]
    }
   ],
   "source": [
    "mmlu_df = fetch_file('pegasus_mmlu_train_dataset.json')\n",
    "print(\"Len is: \", len(mmlu_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ec933484",
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_arr_3 = [\\\n",
    "  #'high_school_microeconomics',\n",
    " #'econometrics',\n",
    " #'professional_psychology',\n",
    " #'high_school_us_history',\n",
    " #'electrical_engineering',\n",
    " #'college_biology',\n",
    " #'high_school_macroeconomics',\n",
    " #'security_studies',\n",
    " #'anatomy',\n",
    " #'business_ethics',\n",
    " #'college_chemistry',\n",
    " #'virology',\n",
    " #'professional_medicine',\n",
    " #'sociology',\n",
    " #'prehistory',\n",
    " #'medical_genetics',\n",
    " #'human_aging',\n",
    " #'clinical_knowledge',\n",
    " #'marketing',\n",
    " #'world_religions',\n",
    " 'high_school_mathematics',\n",
    " #'machine_learning',\n",
    " 'moral_scenarios',\n",
    " #'high_school_government_and_politics',\n",
    " #'international_law',\n",
    " 'college_mathematics',\n",
    " #'high_school_psychology',\n",
    " #'human_sexuality',\n",
    " #'us_foreign_policy',\n",
    " #'college_medicine',\n",
    " #'philosophy',\n",
    " #'formal_logic',\n",
    " #'college_computer_science',\n",
    " 'moral_disputes',\n",
    " #'high_school_european_history',\n",
    " #'high_school_world_history',\n",
    " #'logical_fallacies',\n",
    " #'global_facts',\n",
    " 'abstract_algebra',\n",
    " #'public_relations',\n",
    " #'high_school_geography',\n",
    " #'computer_security',\n",
    " #'management',\n",
    "# 'high_school_chemistry',\n",
    " #'professional_law',\n",
    " #'high_school_biology',\n",
    " 'high_school_statistics',\n",
    " #'nutrition',\n",
    " 'high_school_physics',\n",
    " 'college_physics',\n",
    " #'jurisprudence',\n",
    " #'astronomy',\n",
    " #'high_school_computer_science',\n",
    " #'miscellaneous',\n",
    " #'professional_accounting'\\\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "08774013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  124\n"
     ]
    }
   ],
   "source": [
    "mmlu_df = list(filter(lambda x:x['category'] in _use_arr_3, mmlu_df))\n",
    "print(\"Len is: \", len(mmlu_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "69a1aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mmlu_id': 4401,\n",
       " 'group_id': 1508,\n",
       " 'category': 'high_school_mathematics',\n",
       " 'perturb_type': 'original',\n",
       " 'split_used': 'train',\n",
       " 'instruction': 'The following are multiple choice questions (with answers) about high school mathematics.\\n\\nQuestion: The variable $x$ varies directly as the square of $y$, and $y$ varies directly as the cube of $z$. If $x$ equals $-16$ when $z$ equals 2, what is the value of $x$ when $z$ equals $\\\\frac{1}{2}$?\\nA. -1\\nB. 16\\nC. -\\\\frac{1}{256}\\nD. \\\\frac{1}{16}\\nAnswer:',\n",
       " 'output': 'C'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "704f58d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high_school_mathematics',\n",
       " 'moral_scenarios',\n",
       " 'college_mathematics',\n",
       " 'moral_disputes',\n",
       " 'abstract_algebra',\n",
       " 'high_school_statistics',\n",
       " 'high_school_physics',\n",
       " 'college_physics']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Counter([x['category'] for x in mmlu_df]).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1585a5",
   "metadata": {},
   "source": [
    "#### Constructing the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c4cb18fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1697, 2580, 6000, 919, 124)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lens = len(bigbench_stem_df), len(mathqa_df), len(gsm_df), len(bigbench_df), len(mmlu_df)\n",
    "all_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2c7770ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11320"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(all_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38b13ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec1a6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = bigbench_stem_df + mathqa_df + mmlu_df  + gsm_df[:IDX]\n",
    "random.shuffle(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a480d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =  bigbench_df +  gsm_df[IDX:]\n",
    "random.shuffle(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5a4e109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6401"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bd49b63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4919"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f149b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df1 + df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05645cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1204f5b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11320"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49555c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'math_id': 480,\n",
       "  'category': 'number_theory | 5',\n",
       "  'split_used': 'train',\n",
       "  'instruction': 'Given a mathematics problem, determine the answer. Simplify your answer as much as possible.\\n###\\nProblem: The increasing sequence $1,3,4,9,10,12,13\\\\cdots$ consists of all those positive integers which are powers of 3 or sums of distinct powers of 3. Find the $100^{\\\\mbox{th}}$ term of this sequence.\\nAnswer: ',\n",
       "  'output': '981'},\n",
       " {'bb_stem_id': 304,\n",
       "  'section': 'elementary_math_qa|question_with_language_hint',\n",
       "  'task': 'elementary_math_qa',\n",
       "  'subtask': 'question_with_language_hint',\n",
       "  'org_task': False,\n",
       "  'split_used': 'train',\n",
       "  'instruction': '\\nWhat is the answer to the following math word problem, with the given hint?:in a certain egg - processing plant , every egg must be inspected , and is either accepted for processing or rejected . for every 96 eggs accepted for processing , 4 eggs are rejected . if , on a particular day , 12 additional eggs were accepted , but the overall number of eggs inspected remained the same , the ratio of those accepted to those rejected would be 99 to 1 . how many q eggs does the plant process per day ?\\nsubtract 96 from 99,  divide 12 by the result. multiply result by 100,\\nA. 400\\nB. 4000\\nC. 100\\nD. 3000\\nE. 300\\nAnswer: ',\n",
       "  'output': 'A'},\n",
       " {'bb_id': 540,\n",
       "  'section': 'logical_deduction|three_objects',\n",
       "  'task': 'logical_deduction',\n",
       "  'subtask': 'three_objects',\n",
       "  'org_task': True,\n",
       "  'split_used': 'train',\n",
       "  'instruction': 'The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\nOn a branch, there are three birds: a robin, an owl, and a hawk. The robin is to the right of the hawk. The owl is the leftmost.\\nA. The robin is the rightmost.\\nB. The owl is the rightmost.\\nC. The hawk is the rightmost.\\n ',\n",
       "  'output': 'A'},\n",
       " {'gsm_id': 597,\n",
       "  'category': 'gsm8k_task',\n",
       "  'split_used': 'train',\n",
       "  'verbose': False,\n",
       "  'instruction': 'Q: Ali had a collection of seashells. He started with 180 seashells. He then gave away 40 seashells to his friends. He also gave 30 seashells to his brothers. If he sold half of the remaining seashells, how many seashells did he have left?\\nA: ',\n",
       "  'output': '55'},\n",
       " {'math_id': 2062,\n",
       "  'category': 'geometry | 1',\n",
       "  'split_used': 'train',\n",
       "  'instruction': 'Given a mathematics problem, determine the answer. Simplify your answer as much as possible.\\n###\\nProblem: A square and four circles, each with a radius of 5 inches, are arranged as shown. What is the area, in square inches, of the square? [asy]\\nunitsize(1mm);\\ndefaultpen(linewidth(0.7pt));\\ndraw((0,0)--(20,0)--(20,20)--(0,20)--cycle);\\ndraw(Circle((5,5),5));\\ndraw(Circle((15,5),5));\\ndraw(Circle((5,15),5));\\ndraw(Circle((15,15),5));\\n[/asy]\\nAnswer: ',\n",
       "  'output': '400'},\n",
       " {'math_id': 1659,\n",
       "  'category': 'prealgebra | 2',\n",
       "  'split_used': 'train',\n",
       "  'instruction': 'Given a mathematics problem, determine the answer. Simplify your answer as much as possible.\\n###\\nProblem: Find: $\\\\frac{2}{7}+\\\\frac{8}{10}$\\nAnswer: ',\n",
       "  'output': '\\\\frac{38}{35}'},\n",
       " {'bb_id': 592,\n",
       "  'section': 'logical_deduction|three_objects',\n",
       "  'task': 'logical_deduction',\n",
       "  'subtask': 'three_objects',\n",
       "  'org_task': True,\n",
       "  'split_used': 'train',\n",
       "  'instruction': 'The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\nIn an antique car show, there are three vehicles: a convertible, a tractor, and a hatchback. The hatchback is older than the convertible. The tractor is the newest.\\nA. The convertible is the oldest.\\nB. The tractor is the oldest.\\nC. The hatchback is the oldest.\\n ',\n",
       "  'output': 'C'},\n",
       " {'gsm_id': 3100,\n",
       "  'category': 'gsm8k_task',\n",
       "  'split_used': 'train',\n",
       "  'verbose': False,\n",
       "  'instruction': 'Q: Grant has four times as many vacations as Kelvin has classes. If Kelvin has 90 classes, how many vacations and classes do Grant and Kelvin have altogether?\\nA: ',\n",
       "  'output': '450'},\n",
       " {'gsm_id': 6176,\n",
       "  'category': 'gsm8k_task',\n",
       "  'split_used': 'train',\n",
       "  'verbose': False,\n",
       "  'instruction': 'Q: Méliès bought 2 kg of meat. The meat costs $82 per kilogram. Méliès has $180 in his wallet. How much money does he have left after paying for the meat?\\nA: ',\n",
       "  'output': '16'},\n",
       " {'gsm_id': 6992,\n",
       "  'category': 'gsm8k_task',\n",
       "  'split_used': 'train',\n",
       "  'verbose': False,\n",
       "  'instruction': 'Q: The novelty shop on the Starship Conundrum sells magazines and chocolates.  The cost of four chocolate bars is equal to the cost of 8 magazines.  If one magazine costs $1, how much does a dozen chocolate bars cost, in dollars?\\nA: ',\n",
       "  'output': '24'}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "84a98879",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../data/new_training_datasets/pegasus_combined_math_train_dataset.json\"\n",
    "with open(save_path, 'w') as fd:\n",
    "    json.dump(final_df,fd, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wizard_coder_kernel",
   "language": "python",
   "name": "wizard_coder_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
